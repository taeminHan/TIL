> 교차 엔트로피 오차

---

어제는 오차제곱합을 공부했다면 오늘은 교차 엔트로피 오차 이다.

교차 엔트로피의 수식은 다음과 같다.

$$E = -\sum_k t_k\log y_k$$

이 식에서 log는 밑이 *e*인 자연로그(*loge*)이다.

*`yk`*는 신경망 출력, *`tk`*는 정답 레이블이다.

또한 tk는 정답에 해당하는 인덱스의 원소만 1이고 나머지는 0이다. 

이것은 어제 공부했던 원-핫 인코딩이다.

그래서 위 식은 실질적으로 정답일 때의 추정(`*tk*`가 1일 때의 `*yk*`)의 자연로그를 계산하라는 식이 된다.

정답이 아닌 나머지 모두는 `*tk*`가 0이므로 `*log yk*`와 곱해도 0이 되어 결과에 영향을 주지 않는다.

즉, 교차 엔트로피 오차는 정답일 때의 출력이 전체 값을 정하게 된다!

---

정답에 해당하는 출력이 커질수록 0에 다가가다가, 그 출력이 1일 때, 0이 된다. 반대로 정답일 때의 출력이 작아질수록 오차는 커진다.

```python
def cross_entropy_error(y, t):
	delta = 1e-7
	return -np.sum(t * np.log(y + delta))
```

여기에서 y와 t는 넘파이 배열이다.

코드 마지막에 있는 np.log를 계산할 때 아주 작은 값인 delta를 더했다.

그 이유는 np.log() 함수에 0을 입력하면 마이너스 무한대를 뜻하는 -inf가 되어 더 이상 계산을 진행할 수 없게 되기 때문이다.

그래서 아주 작은 값을 더해서 절대 0이 되지 않도록, 마이너스 무한대가 발생하지 않도록 방지하는 대책법이다!
