## 기울기를 잘 이용해 최소값을 찾으려는 것이 경사법이다.

신경망 역시 최적의 매개변수(**가중치**와 **편향**)를 학습 시에 찾아야 한다.

여기에서 **최적**이란 **손실 함수**가 최솟값이 될 때의 매개변수 값이다.

그러나 일반적으로 손실 함수는 매우 복잡하다.....

매개변수 공간이 광대하여 어디가 최솟값이 되는 곳인지를 짐작할 수 없다. 

각 지점에서 함수의 값을 낮추는 방안을 제시하는 지표가 **기울기**이다.

기울기가 가리키고 있는 곳을 정말 함수의 최솟값이 있다고는 보장할 수 없다..

실제로 복잡한 함수에서는 기울기가 가리키는 방향에 최소값이 없는 경우가 대부분이다.

기울어진 방향이 꼭 최솟값을 가리키는 것은 아니나, 그 방향으로 가야 함수의 값을 줄일 수는 있다.

그러므로 기울기 정보를 단서로 나아갈 방향을 정해야 한다.

함수의 값을 점차 줄이는 것이 경사법**(gradient method)**이다.

경사법은 기계 학습을 최적화하는 데 쓰는 방법이다.

경사법을 수식으로 나타내어 보자

$$x_0 = x_0 - \eta * \frac{\delta f}{\delta x_0}$$

$$x_1 = x_1 - \eta * \frac{\delta f}{\delta x_1}$$

$\eta$ 기호는 갱신하는 양을 나타낸다.  이를 신경망 학습에서는 **학습률(learning rate)**라고 한다.

한번의 학습으로 얼마만큼 학습해야 할지, 즉 매개변수 값을 얼마나 갱신하느냐를 정하는 것이 학습률이다. 

또, 위에서는 변수가 2개인 경우를 보여줬지만, 변수의 수가 늘어도 같은 식으로 갱신하게 된다.

같은 식이란 각 변수의 편미분 값을 말한다.

일반적으로 학습률 값이 너무 크거나 작으면 '**좋은 장소'**를 찾아갈 수 없다. 

신경망 학습에서는 보통 이 학습률 값을 변경하면서 올바르게 학습하고 있는지를 확인하면서 진행한다.

```python
def gradient_descent(f, init_x, lr=0.01, step_num=100):
    x = init_x

    for i in range(step_num):
        grad = numerical_gradient(f, x)
        x -= lr * grad
    return x
```

### 예제: 경사법으로 $f(x_0, x_1) = x_0^2 + x_1^2$의 최소값 구하기

```python
def function_2(x):
    return x[0] ** 2 + x[1] ** 2

init_x = np.array([-0.3, 4.0])
print(gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100))
# [-6.11110793e-11  8.14814391e-10]
```

### 결과값: $[-6.11110793e-11  8.14814391e-10]$

---

### 추가적인 실습 해보기

### 학습률이 너무 큰 예 :  $lr = 10.0$

```python
init_x = np.array([-0.3, 4.0])
print(gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100))
```

### 결과값: $[ 7.35866866e+10 -1.26893162e+12]$

학습률이 너무 크면 큰 값으로 발산해버림

---

### 학습률이 너무 작은 예 :  $lr = 1e-10$

```python
init_x = np.array([-0.3, 4.0])
print(gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100))
```

### 결과값: $[ 7.35866866e+10 -1.26893162e+12]$

학습률이 너무 작으면 거의 갱신되지 않고 끝남

---